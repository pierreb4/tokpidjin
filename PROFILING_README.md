# DSL Function Profiling Guide

This guide explains how to profile DSL function calls in generated batt functions to identify GPU acceleration targets.

## ‚ö†Ô∏è Important: Profile on Kaggle, Not Locally

**Local profiling (laptop) is unreliable** because:
- Threading/multiprocessing in run_batt.py obscures individual DSL function times
- No GPU available to test actual GPU code paths
- Different execution model than production environment

**Always profile on Kaggle** with:
- GPU enabled (T4x2, P100, or L4x4)
- Real training data
- Production batt functions (generated by card.py)

## Profiling Tools

### 1. `profile_batt_dsl.py` - Single Task Profiler

Profile a single batt execution on one task to see detailed DSL function breakdown.

**Usage:**
```bash
python profile_batt_dsl.py -f <batt_module> -t <task_id>
```

**Example:**
```bash
# Profile test_prof_batt.py on task 007bbfb7
python profile_batt_dsl.py -f test_prof_batt -t 007bbfb7
```

**Output:**
- Per-function call counts
- Cumulative times
- Percentage of total execution
- GPU priority recommendations (HIGH/MEDIUM/LOW)

### 2. `profile_batt_batch.py` - Multi-Task Aggregate Profiler

Profile a batt file across multiple tasks to get aggregate statistics. **This is the primary tool for identifying GPU acceleration targets.**

**Usage:**
```bash
python profile_batt_batch.py -f <batt_module> -n <num_tasks> [--start <index>]
```

**Example:**
```bash
# Profile tmp_batt_onerun_run.py across 50 tasks
python profile_batt_batch.py -f tmp_batt_onerun_run -n 50

# Profile next 50 tasks (tasks 50-99)
python profile_batt_batch.py -f tmp_batt_onerun_run -n 50 --start 50

# Save results to JSON
python profile_batt_batch.py -f tmp_batt_onerun_run -n 100 -o profiling_results.json
```

**Output:**
- Aggregate DSL function statistics across all tasks
- Total calls, total time, average per call, average per task
- Percentage of total execution time
- GPU acceleration recommendations with expected impact
- Projected speedup calculations (3-6x for HIGH priority, 2-4x for MEDIUM)

## Kaggle Workflow

### Step 1: Deploy Profilers to Kaggle

1. **Upload files to Kaggle notebook:**
   - `profile_batt_dsl.py`
   - `profile_batt_batch.py`
   - `tmp_batt_onerun_run.py` (or other generated batt file)

2. **Ensure GPU is enabled:**
   - Settings ‚Üí Accelerator ‚Üí GPU T4 x2 (or P100/L4x4)

3. **Install dependencies:**
   ```python
   # Already available in Kaggle environment:
   # - numpy, cupy (for GPU)
   # - standard library (json, cProfile, etc)
   ```

### Step 2: Run Batch Profiler

```python
# In Kaggle notebook cell:
!python profile_batt_batch.py -f tmp_batt_onerun_run -n 100 -o results.json
```

**Recommended task counts:**
- **Quick test**: 20 tasks (~30 seconds, validates setup)
- **Standard profiling**: 50-100 tasks (~2-5 minutes, good statistics)
- **Comprehensive**: 200-400 tasks (~10-20 minutes, production-level data)

### Step 3: Analyze Results

Look for:

1. **HIGH PRIORITY functions (>10% of execution time)**
   - These are the primary GPU acceleration targets
   - Expected 3-6x speedup with GPU
   - Typical candidates: `o_g`, `objects`, `objects_t`

2. **MEDIUM PRIORITY functions (5-10% of execution time)**
   - Secondary targets after HIGH priority implemented
   - Expected 2-4x speedup with GPU
   - Typical candidates: `fgpartition`, `partition`, `gravitate`

3. **LOW PRIORITY functions (1-5% of execution time)**
   - Nice to have, lower ROI
   - Implement only after HIGH/MEDIUM complete

### Step 4: Validate Findings

Run profiler multiple times with different batt files to confirm:
- Consistent bottlenecks across different generated solvers
- Functions appear in multiple runs
- Percentage times are stable (¬±2-3%)

## Expected Results

Based on GPU_SOLVER_STRATEGY.md, we expect:

**For typical generated batt functions:**
- `o_g` / `objects`: 15-30% of execution time
- `fgpartition` / `partition`: 5-15% of execution time
- `gravitate`: 3-10% of execution time
- Other DSL ops: <5% individually

**Total execution times:**
- Simple tasks: 10-50ms
- Medium tasks: 50-200ms
- Complex tasks: 200ms-2s
- Very complex: 2-20s (tasks like 0607ce86, 05a7bcf2)

**Expected GPU impact:**
- HIGH priority functions GPU-accelerated: 3-6x faster
- MEDIUM priority functions GPU-accelerated: 2-4x faster
- Overall solver speedup: 2-4x (weighted average)
- Pipeline speedup: 1.5-2.5x (solvers are 91% of time)

## Implementation Priority

After profiling on Kaggle:

1. **Identify top 3-5 functions** by % of total execution time
2. **Check existing GPU implementations** in dsl.py
3. **Implement GPU versions** using hybrid approach:
   - Arrays/tuples on GPU for computation
   - Convert to frozenset only at function boundaries
   - Maintain CPU fallback for small inputs
4. **Test on Kaggle** with GPU enabled
5. **Measure actual speedup** vs expectations
6. **Iterate** on next highest priority function

## Troubleshooting

### "No significant DSL function calls detected"

**Cause**: Profiler threshold too high or threading obscuring times

**Solutions:**
- Lower threshold in `profile_batt_batch.py` (change `> 0.5` to `> 0.1`)
- Ensure running on Kaggle with GPU (not local laptop)
- Try profiling more complex tasks (use `--start` to skip simple tasks)

### "CuPy not available"

**Cause**: GPU not enabled or CuPy import failed

**Solutions:**
- Enable GPU in Kaggle settings
- Check GPU is actually allocated: `!nvidia-smi`
- Verify CuPy installed: `import cupy as cp; print(cp.cuda.runtime.getDeviceCount())`

### Execution times seem too fast/slow

**Cause**: JIT compilation, warmup, or caching effects

**Solutions:**
- First few tasks may be slower (JIT warmup)
- Look at median times, not first task
- Run 50+ tasks to get stable averages

## Files

- `profile_batt_dsl.py` - Single task profiler (229 lines)
- `profile_batt_batch.py` - Batch profiler with aggregation (274 lines)
- `tmp_batt_onerun_run.py` - Generated batt file (example, 1953 lines)
- `PROFILING_README.md` - This file

## Next Steps

1. ‚úÖ Commit profilers to repo
2. üîÑ Deploy to Kaggle with GPU enabled
3. ‚è≥ Run `profile_batt_batch.py` on 100+ tasks
4. ‚è≥ Analyze results, identify top DSL bottlenecks
5. ‚è≥ Implement GPU-accelerated versions
6. ‚è≥ Validate speedup on Kaggle
7. ‚è≥ Update documentation with actual results

---

**Last Updated**: October 15, 2025  
**Status**: Profilers ready for Kaggle deployment  
**Current Focus**: Getting real profiling data from Kaggle to validate GPU acceleration targets
